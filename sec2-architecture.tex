\section{The Architecture of the Science Platform}\label{architecture}

This section presents the overall architecture of the LSST Science Platform
(LSP, not to be confused with the Science Pipelines "stack" code), including
its major components, interfaces, and design principles.

\subsection{Design Overview}\label{design-overview}

The LSP is a multi-tier architecture with distinct but inter-related
user-facing ``Aspects'' over an underlying layer of services that in turn
rests on computational infrastructure.

\subsubsection{Functional Architecture}\label{functional-architecture}

The Portal Aspect, Notebook Aspect, and Web API Aspects provide three
distinct user interfaces.
Internally, the LSP interfaces with the LSST Data Facility (LDF)'s Data
Backbone via the Data Butler Python client interface and direct
``native'' Data Backbone
interfaces to provide access to the Science Data Archive and (for some
instances) other data products.
The LSP also has its own data storage for
Portal configurations, user notebooks, and other user files and databases.
It interfaces with compute provisioning services, authentication and
authorization services, and resource management, including a proposal
management system to handle requests for resources.
Networking connects the LSP with internal
systems and the external world and provides a measure of cybersecurity.

The Portal Aspect is implemented using the language-agnostic Web APIs to
retrieve data.
Users of the Notebook Aspect can also use the Web APIs if they
desire, either through a VO client library (e.g., \texttt{pyvo},
\texttt{astroquery}) or directly as a Web service.

\subsubsection{Deployment Architecture}\label{deployment-architecture}

The LSP is designed to be deployable in multiple locations within the LSST
Data Management System (DMS). The production instances are the US Data
Access Center, the Chilean Data Access Center, the Science Validation
instance in the NCSA Enclave, and the Commissioning Cluster instance in the
Base Enclave. Additional instances are used for integration testing in the
Integration environment (the Prototype Data Access Center) and for science
pipelines developer usage in the Developer environment. LSP developers
will of course instantiate components of the LSP during their own development.

The LSP is composed of multiple services that are containerized and deployed
using Kubernetes.  The underlying provisioning of compute resources is
handled by the Provisioning and Deployment Service component of the LSST DMS.
 The technology to be used for provisioning in this component is being
evaluated and may vary between instances.  vSphere, HTCondor, SLURM, and
OpenStack are all potential alternatives.  Provisioning is elastic: JupyterLab
dynamically instantiates notebooks on demand, and the Portal and Web API
services can be expanded or contracted as needed. This elasticity, together
with fault tolerance considerations, require that the services hold minimal
non-persistent state so that loss of a single service instance has
correspondingly minimal impact on users.

The LSST Data Facility (LDF) will operate production and development clusters
of the LSP services. The production LSP services are deployed onto the
Kubernetes platform by LDF service production staff. The containers are
deployed using images in local container repositories. There are no external
dependencies so that in the case of network outages at a site containers can
still be deployed, even if the Kubernetes infrastructure needs to be
rebuilt. Batch production services are co-located with LSP service
instances. Data Backbone endpoint services are set up locally to the Base and
the LDF.

Containers to be deployed to Kubernetes are built using the automated build
system.  The containers are placed into local repositories on Kubernetes test
systems vetted by security staff, and then deployed to the production
container registry. Security and monitoring software are put in place to
supervise the activity of the host systems and containers.

Kubernetes releases happen in regular and frequent, intervals. LDF staff
monitor these releases and evaluate the appropriateness of upgrading,
balancing any new features we might be able to take advantage of against any
disruption or change in usersâ€™ workflow.

There are two types of updates to the software that will take place:  
Kubernetes and LSP services.  Updates will be implemented on two test
clusters. The first cluster will be used by system administration to test 
configurations and system software updates.  The second is used for LSP 
service development.

Updates to Kubernetes will first be done on the system administration cluster
to verify update procedures. After this has been successfully completed, the
update will be done to the LSP development cluster, and regression tests for
LSP services will take place.  After this has been verified,  Kubernetes will
be upgraded on the production cluster.

Updates to LSP services are first done on LSP service development cluster. 
After all tests have passed, the LSP service updates will be done on the 
production cluster.

\subsection{Data Access and Storage}\label{data-access-and-storage}

Since the goal of the LSP is to retrieve and analyze LSST data products,
data access and the storage of user data are key aspects of the platform.

\subsubsection{Databases}\label{databases}

Since the LSST catalog data products represent the best measurements of
astrophysical phenomena available to the project as well as metadata
describing how data was taken and processed, it is anticipated that
most uses of the LSP will involve these catalogs, which are stored in
databases.

\paragraph{Database content overview}\label{database-content-overview}

Several categories of database data will be accessible to the LSP instances:
\begin{itemize}
\item Image and visit metadata
\item Catalogs
\item Composite data (binary data in catalogs)
\item Observatory metadata (including EFD)
\item Reference catalogs
\end{itemize}

Image and visit metadata includes information about the observation captured at
the time of imaging.
It also includes information derived later from analyzing the data
and metadata, such as an accurate mapping from pixel coordinates in an image to
sky coordinates or an estimate of the photometric zeropoint in an image.
The metadata will follow the Common Archive Observation
Model, version 2 \citep{2012ASPC..461..339D,CAOM2}, as its core.
This metadata will be stored in relational databases and made available
via ADQL queries.

Catalogs are described in detail in the Data Products Definition Document
\citeds{LSE-163}.  They include summary measurements of astrophysical Objects,
plus measurements in images from each epoch of observation and in difference
images.  These will also be stored in relational databases and made available
via ADQL queries.  The catalog of all Alerts that have been issued may be
stored in a different database technology, such as a document database, and
may thus use a different query interface.

The above catalogs will include ``BLOB'' fields that provide large data items
that are useful to retrieve along with other catalog data but are not
suitable for searching.  Examples include samples of the posterior probability
surface for a model, photometric redshift probability distribution functions,
or ``heavy footprints'' giving deblended pixel values for an Object.  These
are generally stored in separate tables within the relational database as
they are used less frequently.

The Engineering and Facility Database, which contains a time history of all
commands, configuration, and telemetry from the Observatory systems, will
be transformed to provide simplified query access to conditions associated
with specific observations.
All data present in the original EFD will be available in the Transformed EFD
with the exception of specific Internal, Sensitive, or Highly Sensitive
information, as defined in the Information Classification Policy
\citeds{LPM-122},
which will not be available to users other than project staff.
(The EFD will not contain any Protected User Data.)
The Transformed EFD will be stored in a relational database,
accessible via ADQL queries.

The Large File Annex (LFA) of the EFD will be carried over into the Transformed
EFD, and its contents will be accessible from the Science Platform.
In practice this means that URIs retrieved from the EFD tables that index the
LFA must be resolvable in all three Aspects.

Reference catalogs used in the LSST Alert, Calibration Products, and Data
Release Productions will be provided, also in relational databases.
Additional catalogs, e.g.,\ from precursor or concurrent surveys in a variety
of wavelengths, will also be provided as relational databases depending on
usefulness to the Science Pipelines or the validation of Data Products,
and available resources.
In particular, it is expected that releases of the Gaia data will be provided.

\paragraph{Database technologies}\label{database-technologies}

While many of the above databases will be implemented in ``conventional''
relational database technology, the largest catalogs will require a distributed
system to provide sufficient performance.
The LSST-developed Qserv distributed database will be used to implement these.
These two technologies may vary somewhat in their support of ADQL features.
Users creating their own catalogs (see section~\ref{user-catalog-data-support})
will need to specify whether they are to be
distributed, thereby selecting the appropriate back-end implementation.

In addition to this, the Data Access team is exploring the possibility of
hosting a replica of selected catalog data in a non-database form in order to
facilitate analysis with contemporary data-science workflow systems, e.g.,
in the form of Apache Parquet files.
This work is being done in the context of fulfilling the requirement to
define the long-anticipated ``Next to Database Analysis'' system that was
mooted in the original DM designs.

\subsubsection{Images}\label{images}

LSST image data products include raw images, processed visit images, difference
images, coadded images, and templates.  These are part of the Science Image
Archive in the Data Backbone.  All of these are accessible to the LSP.

Some data products are currently specified to be ``virtual'': they are
regenerated on demand from other persistently-stored data products.
The choice of which Data Products to treat in this matter arises from
estimates of the technology-dependent space/time cost trade-offs and as
such may be revisited during the Operations era as technology evolves.
Such virtual data products are regenerated for the LSP by services that are
part of the LSP Web APIs.

Images will be delivered in at least FITS format; other formats may be added.
The LSST DM internal image data format is currently FITS as well, but that may
change in the future.  Any necessary translation between internal format and
external format is performed by services that are part of the LSP Web APIs.

Image data products typically contain additional information besides image
pixels.  This information includes metadata describing the conditions under
which the image was generated, its provenance and processing history, if any,
and metadata derived from the image itself.  Much of this metadata can be
expressed as simple data types in "headers" or similar mechanisms, but some,
such as mask and variance planes, pixel-to-sky mappings, and point spread
function (PSF) models, may be too complex for such a representation.  When this
information is specific to the image, LSST DM generally stores this information
in the same file as the image pixels, as additional pixel planes or extension
tables.  The structure and content of these will be documented for LSP users.

\subsubsection{LSST-specific Objects}\label{lsst-specific-objects}

The Python objects used by the Science Pipelines code do not always
correspond one-to-one with persisted catalog or image data products in the
Data Backbone.
For example, an image and its background model may be persisted
in two distinct files, while represented by a single Python class.
The PSF model associated with the detections in an
image may be stored in a file while the information about the detections is
stored in a database catalog.

The component files and catalogs are directly accessible via the LSP, but
for convenience the Data Butler client library provides mechanisms to define
composites that combine multiple components into a single retrieved Python
object.  Since this usage is specific to the Python language, it is only
directly visible to the user in the LSP in the Notebook Aspect.

\subsubsection{Data Access Services}\label{data-access-services}

The LSP Web API Aspect provides services to retrieve catalog and image data
products.
Access to catalog data products is provided by a Web service that
supports the VO TAP protocol \citep{2010ivoa.spec.0327D} with queries specified
in ADQL \citep{2008ivoa.spec.1030O}.
Access to image
and file data products is provided by a separate Web service that supports the
VO SIA \citep{2015ivoa.spec.1223D}, SODA, and VOSpace \citep{2013ivoa.spec.0329G} protocols.
The image and file service is also
responsible for regenerating ``virtual'' data products and converting from
internal to external data formats as necessary.
Both of these Web services
support asynchronous operations, as some requests will have long response
latencies (e.g.,\  large-scale catalog queries or data regeneration).
Access to
metadata about images and files is provided by TAP/ADQL queries on metadata
tables, rather than a distinct metadata service.  More detail on these services
is provided in Section~\ref{api-aspect} below.

Within the Notebook Aspect, and internal to the Web API Aspect, the Data
Butler client interface may be used to retrieve Python objects, and a
``native'' Data Backbone API may be used for high-performance retrieval of
datasets.
The Portal Aspect primarily uses the Web API Aspect for data access,
though the Data Butler and ``native'' Data Backbone API may be used in Python
plugins that provide support for access to, and visualization of, LSST-specific
complex datatypes.

\subsubsection{User Catalog Data Support}\label{user-catalog-data-support}

Users may create their own catalogs within the LSP, a capability that is
patterned after SDSS SkyServer's ``MyDB''.
Catalogs may be derived from queries
to the LSST catalogs or loaded from computations or external data sources.  As
mentioned above in section~\ref{database-technologies}, large distributed
catalogs meant for joining with the LSST data products must be identified by
the user so that they can be stored appropriately in the Qserv database.

All three Aspects will provide mechanisms for creating user catalogs; the
Portal's mechanism will be implemented via the Web APIs, as usual.
User catalogs may be accessed in the same ways that LSST catalog data products
are accessed, and are visible in every Aspect.
They may be shared with other users, as discussed further in
\S~\ref{data-access-permissions-and-quotas}.

User catalogs are backed up for disaster recovery purposes, but they are not
fully protected against user error.

\subsubsection{User File Workspace Storage}\label{user-workspace-storage}

Users may create their own images and other files within the LSP.
These all live within a User Workspace that is common across all three Aspects.
Files in this Workspace may be shared with other users, as discussed further in
\S~\ref{data-access-permissions-and-quotas}.

All three Aspects will provide mechanisms for creating and retrieving user
files in this Workspace.
In the Notebook Aspect, this is via a mounted filesystem.
In the Web API Aspect, this is via VOSpace (and most likely also WebDAV).
The Portal Aspect will provide a file-browsing interface, will offer
visualizations for certain recognized file types, such as image and table
FITS files and common textual tabular file formats such as CSV, and will
facilitate the download and upload of files into/from the User File Workspace.
We will investigate whether the Portal and Notebook Aspect can share the
file browsing and visualization tools provided by JupyterLab.

Users' top-level home directories in the Unix environment provided in the
Notebook Aspect (underneath the notebook Python kernels and via JupyterLab's
shell services) will \emph{not} be part of the User File Workspace \emph{per
se} and so will not be visible via VOSpace.
This is motivated by both security considerations and to provide flexibility
for the Notebook Aspect in implementation.
We expect that the filesystem mount of the User File Workspace will be
linked as a standard subdirectory of the user's home directory.

We currently do not expect that the User File Workspace will be shared
across DACs.
Specifically, when logged in to the Notebook Aspect, the mounted User File
Workspace seen by the user will be local to the Data Access Center.
However, since the User File Workspace from each Science Platform instance
is globally accessible via the VOSpace protocol in the API Aspect, it will be
possible for a user at one DAC to get access \emph{via VOSpace}
to their workspace from another DAC
(presuming that they have access rights at both DACs in the first place),
albeit perhaps at reduced performance compared to the DAC-local workspace.

All users will have a basic quota of User File Workspace storage, and may then
apply to a resource allocation committee for additional storage for specific
purposes.

User file workspaces are backed up for disaster recovery purposes, but they are
not fully protected against user error.

\subsubsection{Data Access Permissions and Quotas}\label{data-access-permissions-and-quotas}

All LSST data products are available to all users with LSST data rights.
In order to facilitate collaboration,
users may grant other users or groups of users access to the personal data
they have created in their User File and/or Database Workspaces.
Such access may be read-only or read-write.

The LSST authentication, authorization, and identity management tools will
permit users to create and manage groups on their own, without support requests
requiring human intervention.

The system is designed with the flexibility to permit data rights to be
granted per Data Release.
This permits, for instance, the implementation of policies such as permitting
users who previously had LSST data rights but have since lost them
(e.g.,\ due to a change in national or institutional affiliation)
to continue to be granted access to Data Release data products
available as of the time of loss, but not newer ones.
(This is an example only; the actual policies will be determined by the
Operations organization.)

Per-user catalogs and files are a limited resource with quotas determined by
resource management (see section~\ref{resource-management}).
Some ``user'' catalogs and files may be considered to be owned by a group,
with a distinct quota.

\subsubsection{Support of Previous Releases}\label{support-of-previous-releases}

The LSST baseline currently anticipates that the data products contained in
at least the most recent two Data Releases,
as well as the Prompt / Level 1 data products from Alert Production,
will be available through the LSST Science Platform.
Data products from previous Data Releases would need to be retrieved via the
Bulk Distribution service; they would not be visible within the LSP.

However, the baseline also requires that the system be scalable to support
keeping all releases loaded in the LSP, subject to the operations organization
providing sufficient physical resources.\footnote{DMS-REQ-0364}
It also states that the system ``should'' continue to support the original
query behavior for as long as a release is loaded.\footnote{DMS-REQ-0371}
(It does not require that queries against older releases automatically be
usable against newer ones; it is recognized that the data model may evolve
over time.)

\subsection{Computing Resources}\label{computing-resources}

The LSP provides access to computing resources to enable users to analyze
LSST data products, alone and in combination with user-provided data.

\subsubsection{Basic user compute services}\label{basic-user-compute-services}

Users of the Portal Aspect share computing resources allocated to the Portal as
a whole.  The same strategy is used for the Web APIs.  Within those shared
resources, per-user accounting is performed for resources such as queries and
temporary disk space so that resource management policies may be implemented
if required.

Each user of the Notebook Aspect is assigned a virtual machine per notebook.
The virtual machine can be selected from a menu of preconfigured installations
of LSST and other software, or users can customize a virtual machine and save
it for future use.  The resources allocated to the virtual machine are
defined by policy.

\subsubsection{Large-scale batch and parallel computing}\label{large-scale-batch-and-parallel-computing}

Users of the LSP (any Aspect) may trigger large-scale, long-latency,
asynchronous data processing.  This processing is handled by submitting jobs to
a batch system based on HTCondor.  Mechanisms are provided in the three Aspects
to monitor the status of batch jobs and retrieve their results.  Instances of
the workload and workflow management tools and services used by the LSST Data
Facility to manage large-scale productions will be made available to each LSP
instance, although these will be separate and distinct from the production
instances, of course.

\subsection{Authentication and Authorization}\label{authentication-and-authorization}

Identity management and associated roles and authorizations are provided by a
unified LSST system as described in LSE-279.  This system allows the use of
external identity providers such as the Chilean COFRe federation \citep{COFRe}
or the US InCommon \citep{InCommon} or GitHub; those verified external
identities are then mapped to internal LSST identities.  All uses of all
instances of the LSP, including the Web API Aspect, are authenticated.

\subsection{Resource Management}\label{resource-management}

Each LSP user in the DACs will receive a certain basic quota of resources
(compute, file storage, database storage, query, bandwidth, etc.).
Usage beyond that quota will be allocated by a resource allocation committee.
A proposal management system (not formally part of the Science Platform itself)
will track requests and allocations that will then be implemented by per-user
resource management features within each LSP component.

\subsection{Cybersecurity Considerations}\label{cybersecurity-considerations}

The two Data Access Center instances of the LSP are the only ones exposed
directly to the public Internet.
The Science Validation, Commissioning Cluster,
Integration, and Developer instances are only reachable from LSST-internal
networks, helping to reduce the attack surface.
The DAC instances are isolated (on a different LAN) from the production
Level~1 and Level~2 domains within the NCSA enclave.
Although they may share underlying computing resources, such
resources are provisioned for either production or DAC but not both at the
same time.

The LSST data products within the Data Backbone will be read-only to the DAC
LSP instances.
The Data Backbone interfaces are all authenticated,
but they will be used for some types of user data, so the Backbone must be
hardened against attack (or accident) from the LSP.
Provisioning separate virtual machines per user in the Notebook Aspect helps
to prevent unwanted sharing of information between users.
All the Aspects
need to be hardened in the same ways as any Web-enabled service, including
sanitizing user inputs, avoiding cross-site scripting, etc.

All Web access services available from the public Internet will use the HTTPS
protocol, unless specific exceptions are formally authorized under change
control and with the concurrence of the Project ISO.
(This would likely only arise if specific substantial and measurable
performance considerations motivated it.)

\subsection{Additional Support Services}\label{additional-support-services}

System management and monitoring services for the LSP will be provided by the
infrastructure layer supporting each instance.  Monitoring at the system and
application level will be exposed to LSP users where consistent with user
privacy and system security.
